# Chat-to-3D All-in-One Docker - Dosya YapÄ±sÄ±

Bu dokÃ¼mantasyon, all-in-one container Ã§Ã¶zÃ¼mÃ¼ iÃ§in oluÅŸturulan dosyalarÄ± aÃ§Ä±klar.

## ğŸ“ Dizin YapÄ±sÄ±

```
allinone/
â”œâ”€â”€ Dockerfile                  # All-in-one container tanÄ±mÄ±
â”œâ”€â”€ docker-compose.yml          # Tek container orchestration
â”œâ”€â”€ supervisord.conf           # Servis yÃ¶netimi (3 servis)
â”œâ”€â”€ start.sh                   # Container baÅŸlangÄ±Ã§ scripti
â”œâ”€â”€ install.sh                 # Otomatik kurulum scripti
â”œâ”€â”€ requirements-extra.txt     # Ekstra Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ .env.example              # Ã–rnek ortam deÄŸiÅŸkenleri
â”œâ”€â”€ .dockerignore             # Docker build ignore
â”œâ”€â”€ .gitignore                # Git ignore
â”œâ”€â”€ README.md                 # DetaylÄ± dokÃ¼mantasyon
â””â”€â”€ QUICKSTART.md             # HÄ±zlÄ± baÅŸlangÄ±Ã§ kÄ±lavuzu

Ana dizin (../):
â”œâ”€â”€ nim_llm/
â”‚   â””â”€â”€ run_llama_local.py    # Local LLM servisi (Transformers)
â””â”€â”€ nim_trellis/
    â””â”€â”€ run_trellis_local.py  # Local TRELLIS servisi (Python)
```

## ğŸ“„ Dosya AÃ§Ä±klamalarÄ±

### Dockerfile

**AmaÃ§:** Tek container imajÄ±nÄ± oluÅŸturur

**Ä°Ã§erik:**

- CUDA 12.8.1 base
- Miniconda kurulumu
- Python 3.11.9 conda ortamÄ±
- TÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kler
- Supervisor kurulumu
- Model indirme (build time)
- Port 7860 expose

**Ã–zellikler:**

- Multi-stage deÄŸil (all-in-one)
- Supervisor ile servis yÃ¶netimi
- GPU desteÄŸi
- Health check dahil

### docker-compose.yml

**AmaÃ§:** Container'Ä± baÅŸlatÄ±r ve yapÄ±landÄ±rÄ±r

**Servis:** `chat-to-3d-allinone`

- Single service tanÄ±mÄ±
- GPU reservation
- Volume mount'larÄ±
- Environment variables
- Health check
- Restart policy

**Volume'lar:**

- `trellis-data`: Uygulama verileri
- `huggingface-cache`: HF model cache
- `torch-cache`: PyTorch cache

### supervisord.conf

**AmaÃ§:** Container iÃ§inde 3 servisi yÃ¶netir

**Servisler:**

1. **llm-service** (Priority 1)

   - `/app/nim_llm/run_llama_local.py`
   - Port: 19002 (internal)
   - Auto-restart

2. **trellis-service** (Priority 2)

   - `/app/nim_trellis/run_trellis_local.py`
   - Port: 8000 (internal)
   - Auto-restart

3. **gradio-app** (Priority 3)
   - `/app/app.py`
   - Port: 7860 (exposed)
   - Auto-restart
   - 30s start delay

**Log YÃ¶netimi:**

- `/var/log/supervisor/`
- Her servis iÃ§in ayrÄ± log dosyalarÄ±
- stdout ve stderr ayrÄ±

### start.sh

**AmaÃ§:** Container baÅŸlangÄ±Ã§ iÅŸlemleri

**Ä°ÅŸlemler:**

1. Conda ortamÄ±nÄ± aktive et
2. Model'leri kontrol et/indir
3. Dizinleri oluÅŸtur
4. Supervisor'u baÅŸlat

### install.sh

**AmaÃ§:** Otomatik kurulum ve baÅŸlatma

**AdÄ±mlar:**

1. Gereksinimleri kontrol et (Docker, GPU, etc.)
2. `.env` dosyasÄ± oluÅŸtur
3. HF token al (opsiyonel)
4. Dizinleri oluÅŸtur
5. Container'Ä± build et
6. Container'Ä± baÅŸlat
7. Health check yap
8. SonuÃ§ bildir

### requirements-extra.txt

**AmaÃ§:** All-in-one iÃ§in ekstra baÄŸÄ±mlÄ±lÄ±klar

**Paketler:**

- `fastapi`: REST API framework
- `uvicorn`: ASGI server

**Neden ayrÄ±:**
Ana requirements'ta yok, sadece local servisler iÃ§in gerekli

### .env.example

**AmaÃ§:** Ortam deÄŸiÅŸkenleri ÅŸablonu

**DeÄŸiÅŸkenler:**

- `HF_TOKEN`: Hugging Face token (Llama iÃ§in)
- `GRADIO_SERVER_NAME`: Gradio host
- `GRADIO_SERVER_PORT`: Gradio port
- `CUDA_VISIBLE_DEVICES`: GPU seÃ§imi

### run_llama_local.py

**AmaÃ§:** Local LLM servisi (NIM replacement)

**Ã–zellikler:**

- FastAPI REST API
- OpenAI-compatible endpoints
- Transformers backend
- Llama 3.1 veya Llama 2 fallback
- GPU auto-detection
- Health check endpoints

**Endpoints:**

- `GET /v1/health/ready`: HazÄ±r mÄ±?
- `GET /v1/health/live`: Ã‡alÄ±ÅŸÄ±yor mu?
- `POST /v1/chat/completions`: Chat API
- `GET /v1/models`: Model listesi

### run_trellis_local.py

**AmaÃ§:** Local TRELLIS 3D generation servisi

**Ã–zellikler:**

- FastAPI REST API
- TRELLIS pipeline
- Image-to-3D conversion
- GLB export
- Content filtering
- GPU acceleration

**Endpoints:**

- `GET /v1/health/ready`: HazÄ±r mÄ±?
- `GET /v1/health/live`: Ã‡alÄ±ÅŸÄ±yor mu?
- `POST /v1/infer`: 3D generation

## ğŸ”„ Ä°ÅŸ AkÄ±ÅŸÄ±

### Build Time (Dockerfile)

```
1. Base image (CUDA + Ubuntu)
2. System dependencies
3. Miniconda install
4. Conda environment create
5. Python packages install
6. App files copy
7. Model download (optional)
8. Directory setup
9. Supervisor config
10. Permissions
```

### Runtime (supervisord.conf)

```
Container Start
    â†“
start.sh
    â†“
Supervisor Start
    â†“
â”œâ”€â†’ llm-service (Priority 1)
â”‚   â””â”€ run_llama_local.py â†’ Port 19002
â”‚
â”œâ”€â†’ trellis-service (Priority 2)
â”‚   â””â”€ run_trellis_local.py â†’ Port 8000
â”‚
â””â”€â†’ gradio-app (Priority 3)
    â””â”€ app.py â†’ Port 7860
```

### User Request Flow

```
Browser (Port 7860)
    â†“
Gradio App
    â†“
â”œâ”€â†’ LLM Service (Port 19002)
â”‚   â””â”€ Transformers â†’ LLM response
â”‚
â””â”€â†’ TRELLIS Service (Port 8000)
    â””â”€ TRELLIS Pipeline â†’ 3D GLB
```

## ğŸ”§ Servis YÃ¶netimi

### Supervisor Commands (Container iÃ§inde)

```bash
# Durum kontrol
supervisorctl status

# Servis baÅŸlat/durdur
supervisorctl start llm-service
supervisorctl stop trellis-service
supervisorctl restart gradio-app

# TÃ¼m servisleri yeniden baÅŸlat
supervisorctl restart all

# LoglarÄ± izle
supervisorctl tail -f llm-service
supervisorctl tail -f trellis-service stdout
```

### Docker Commands (Host'ta)

```bash
# Container baÅŸlat
docker compose up -d

# LoglarÄ± izle
docker compose logs -f

# Container iÃ§ine gir
docker compose exec chat-to-3d-allinone bash

# Servis durumu (container iÃ§inde)
docker compose exec chat-to-3d-allinone supervisorctl status

# Container'Ä± yeniden baÅŸlat
docker compose restart
```

## ğŸ“Š Resource Usage

### Build Time

- Disk: ~30GB (layers + cache)
- Time: 30-60 dakika (ilk defa)
- Network: ~10GB (downloads)

### Runtime

- GPU VRAM: 10-15GB
- RAM: 8-16GB
- Disk: 50GB (models + cache)
- CPU: 4+ cores Ã¶nerilir

## ğŸ” GÃ¼venlik

### Port Exposure

- **7860**: Public (Gradio UI)
- **19002**: Internal (LLM service)
- **8000**: Internal (TRELLIS service)

### Credentials

- HF_TOKEN: `.env` dosyasÄ±nda
- `.env` git'e commit edilmez (.gitignore)

## ğŸ› Debug

### Log Locations

```
/var/log/supervisor/
â”œâ”€â”€ supervisord.log              # Ana log
â”œâ”€â”€ llm-service.out.log          # LLM stdout
â”œâ”€â”€ llm-service.err.log          # LLM stderr
â”œâ”€â”€ trellis-service.out.log      # TRELLIS stdout
â”œâ”€â”€ trellis-service.err.log      # TRELLIS stderr
â”œâ”€â”€ gradio-app.out.log           # Gradio stdout
â””â”€â”€ gradio-app.err.log           # Gradio stderr
```

### Common Issues

**Model yÃ¼kleme hatasÄ±:**

- Log: `llm-service.err.log`
- Ã‡Ã¶zÃ¼m: HF_TOKEN ekle

**GPU tanÄ±nmÄ±yor:**

- Log: `trellis-service.err.log`
- Ã‡Ã¶zÃ¼m: NVIDIA runtime kontrol

**Port conflict:**

- Log: `gradio-app.err.log`
- Ã‡Ã¶zÃ¼m: Port deÄŸiÅŸtir

## ğŸ“ GeliÅŸtirme

### Yeni Servis Eklemek

1. `supervisord.conf`'a ekle:

```ini
[program:yeni-servis]
command=/opt/conda/bin/conda run -n trellis python /app/yeni_servis.py
directory=/app
autostart=true
autorestart=true
```

2. Container'Ä± rebuild et:

```bash
docker compose build --no-cache
docker compose up -d
```

### Dependency Eklemek

1. `requirements-extra.txt`'e ekle
2. Rebuild:

```bash
docker compose build --no-cache
```

## ğŸ”„ GÃ¼ncelleme

```bash
# Kodu gÃ¼ncelle
git pull

# Rebuild (no cache)
docker compose build --no-cache

# Restart
docker compose down
docker compose up -d
```

## ğŸ“š Ä°lgili Dosyalar

- Ana Docker Compose: `../docker-compose.yml`
- Ana Dockerfile: `../Dockerfile`
- Original install.bat: `../install.bat`
- Config: `../config.py`

---

Bu dokÃ¼mantasyon all-in-one Ã§Ã¶zÃ¼mÃ¼nÃ¼n teknik detaylarÄ±nÄ± iÃ§erir.
KullanÄ±cÄ± dokÃ¼mantasyonu iÃ§in: `README.md` ve `QUICKSTART.md`
